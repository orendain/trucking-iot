# For a list of constants for all the configurations possible on a Storm cluster and Storm topology, check out:
# http://storm.apache.org/releases/current/javadocs/org/apache/storm/Config.html
#
# Default values are available and can be seen here:
# https://github.com/apache/storm/blob/v1.0.2/conf/defaults.yaml

topology {

  # Specifies how many processes you want allocated around the cluster to execute the topology.
  # Each component in the topology will execute as many threads. The number of threads allocated to a given
  # component is configured through the setBolt and setSpout methods. Those threads exist within worker processes.
  # Each worker process contains within it some number of threads for some number of components.
  # For instance, you may have 300 threads specified across all your components and 50 worker processes specified
  # in your config. Each worker process will execute 6 threads, each of which of could belong to a different component.
  # You tune the performance of Storm topologies by tweaking the parallelism for each component and the number of
  # worker processes those threads should run within.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_WORKERS
  workers = 1

  # The maximum amount of time given to the topology to fully process a message
  # emitted by a spout. If the message is not acked within this time frame, Storm
  # will fail the message on the spout. Some spouts implementations will then replay
  # the message at a later time.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS
  message.timeout.secs = 60

  # When set to true, tells Storm to log every message every emitted by a component.
  # This is useful in local mode when testing topologies, but you probably want to keep this turned off
  # when running topologies on the cluster.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_DEBUG
  debug = false

  # How many instances to create for a spout/bolt. A task runs on a thread with zero or more
  # other tasks for the same spout/bolt. The number of tasks for a spout/bolt is always
  # the same throughout the lifetime of a topology, but the number of executors (threads) for
  # a spout/bolt can change over time. This allows a topology to scale to more or less resources
  # without redeploying the topology or violating the constraints of Storm (such as a fields grouping
  # guaranteeing that the same value goes to the same task).
  #
  # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_TASKS
  tasks = 1

  # Bolt-specific configuration for windowed bolts to specify the window length in time duration.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_BOLTS_WINDOW_LENGTH_DURATION_MS
  bolts.window.length.duration.ms = 20000

  # Bolt-specific configuration for windowed bolts to specify the sliding interval as a count of number of tuples.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_BOLTS_SLIDING_INTERVAL_COUNT
  bolts.window.sliding.interval.count = 10


  # The maximum parallelism allowed for a component in this topology. This configuration is
  # typically used in testing to limit the number of threads spawned in local mode.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_MAX_TASK_PARALLELISM
  max.task.parallelism = 1
}

storm {

  # A list of hosts of ZooKeeper servers used to manage the cluster.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.STORM_ZOOKEEPER_SERVERS
  zookeeper.servers = "localhost"

  # The root location at which Storm stores data in ZooKeeper.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.STORM_ZOOKEEPER_ROOT
  zookeeper.root = "/storm"
}

nifi {

  # NiFi URL
  #
  # Config path can be fetched using the constant TruckingTopology.NiFiUrl
  url = "http://sandbox.hortonworks.com:9090/nifi"

  # From NiFi:
  #
  # port-name: NiFi output port name
  # batch-size: Requested NiFI batch count

  truck-data {
    port-name = "Enriched Truck Data"
    batch-size = 10
  }

  traffic-data {
    port-name = "Traffic Data"
    batch-size = 10
  }

  # To NiFi:
  #
  # port-name: NiFi output port name
  # batch-size: NiFI batch size
  # tick-frequency: Tick frequency, in seconds, to push to NiFi

  truck-and-traffic-data {
    port-name = "Enriched Truck And Traffic Data"
    batch-size = 1
    tick-frequency = 1
  }

  driver-stats {
    port-name = "Windowed Driver Stats"
    batch-size = 1
    tick-frequency = 1
  }
}

kafka {

  # A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
  # The client will make use of all servers irrespective of which servers are specified here for bootstrapping.
  # This list only impacts the initial hosts used to discover the full set of servers.
  bootstrap-servers = "sandbox.hortonworks.com:6667,sandbox.hortonworks.com:6667"

  # Serializer class for key that implements the Serializer interface.
  key-serializer = "org.apache.kafka.common.serialization.StringSerializer"

  # Serializer class for value that implements the Serializer interface.
  value-serializer = "org.apache.kafka.common.serialization.StringSerializer"

  # Avro serializer class for value that implements the Serializer interface.
  avro-value-serializer = "com.hortonworks.registries.schemaregistry.serdes.avro.kafka.KafkaAvroSerializer"

  # A unique string that identifies the consumer group this consumer belongs to.
  # This property is required if the consumer uses either the group management functionality by
  # using subscribe(topic) or the Kafka-based offset management strategy.
  #
  # More info at: http://kafka.apache.org/documentation.html#newconsumerconfigs
  group-id = "d"

  # Name of the Kafka topic for truck data.
  truck-data.topic = "trucking_data_truck"

  # Name of the Kafka topic for traffic data.
  traffic-data.topic = "trucking_data_traffic"

  # Name of the Kafka topic for fully joined trucking data.
  joined-data.topic = "trucking_data_joined"

  # Name of the Kafka topic for windowed driver stats.
  driver-stats.topic = "trucking_data_driverstats"
}

hbase {

  #event-key-field = "eventKey"

  # The HBase column family shared by all relevant tables.
  column-family = "c"

  # Name of the HBase table storing the fully joined trucking data.
  trucking-data.table = "trucking_data_joined"
}

schema-registry {
  url = "http://sandbox.hortonworks.com:15005/api/v1"
}
